{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f627c88-8890-44e6-9c8a-8951d4a231fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Analyzing Trends in AI Research Publication\n",
    "# *Full Ingestion from Kaggle*\n",
    "# From Bronze To Silver\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa744149-a2b2-4bf2-9999-b9e9b8cf92da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d00a73-b82e-4e58-80ff-d75199942bae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1d1b1d-3ee8-497d-9d30-f180eb6e24ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35ba45c-bf06-44f6-9606-d7953b14af93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Arxiv Database in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00159cd-c7af-443a-ab9c-74bacc38aa07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the 'arxiv' database exists or create it\n",
    "if not spark.catalog.databaseExists(\"arxiv\"):\n",
    "    spark.sql(\"CREATE DATABASE arxiv\")\n",
    "\n",
    "# Switch to the 'arxiv' database\n",
    "spark.sql(\"USE arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb585d8-f77f-461a-8669-826126dedcb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80860e1-735c-432e-ab65-c895e69ff288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/mnt/arxiv/\"\n",
    "INGESTION_PATH = \"/mnt/arxiv/bronze/kaggle/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a775670-02d0-49f2-a4db-606a29f13e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76546cc0-320b-4081-9570-bbbf863b59c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kaggle_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"submitter\", StringType(), True),\n",
    "    StructField(\"authors\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"journal-ref\", StringType(), True),\n",
    "    StructField(\"doi\", StringType(), True),\n",
    "    StructField(\"report-no\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"license\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True),\n",
    "    StructField(\"versions\", ArrayType(StructType([\n",
    "        StructField(\"created\", StringType(), True),\n",
    "        StructField(\"version\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"update_date\", StringType(), True),\n",
    "    StructField(\"authors_parsed\", ArrayType(ArrayType(StringType())), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa799b9-5ed5-4a75-9400-62bd6c4b2f02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ace4a3a-50a3-4a23-a5c1-d42686a53ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_latest_json():\n",
    "    \"\"\"\n",
    "    Load the most recent JSON file into a DataFrame\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(INGESTION_PATH) \n",
    "    json_files = [f.name for f in files if f.name.endswith('.json')]\n",
    "    sorted_files = sorted(json_files, reverse=True)\n",
    "    latest_file = sorted_files[0]\n",
    "    return spark.read.schema(kaggle_schema).json(INGESTION_PATH + latest_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64cf4a8-8f6d-4585-8095-993ce81f3645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delta_table_exists(layer, table_name):\n",
    "    \"\"\"\n",
    "    Check if Delta table exists\n",
    "    \"\"\"\n",
    "    table_path = f\"{BASE_PATH}{layer}/delta/{table_name}/_delta_log/\"\n",
    "    try:\n",
    "        dbutils.fs.ls(table_path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f454fcfa-0dd2-4d45-a76f-45c02586ec63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_replace_kaggle_delta(layer, table_name, chunk_size=12, recreate=False, join_on=[\"id\"]):\n",
    "    \"\"\"\n",
    "    Function to manage the creation or replacement of a Delta table from the latest JSON file.\n",
    "    This function supports only full ingestion.\n",
    "    \n",
    "    Args:\n",
    "        layer (str): The layer (bronze, silver, gold) where the Delta table resides or will reside.\n",
    "        table_name (str): The name of the Delta table.\n",
    "        chunk_size (int, optional): Number of partitions for chunking the JSON file. Default is 12.\n",
    "        recreate (bool, optional): Whether to recreate the table if it already exists. Default is False.\n",
    "        join_on (list, optional): This parameter is kept for compatibility but is not used in the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    delta_path = f\"{BASE_PATH}{layer}/delta/{table_name}/\"\n",
    "    \n",
    "    # Inform the user that the JSON file is being read\n",
    "    print(\"Reading the latest JSON file in chunks...\")\n",
    "    json_chunks = load_latest_json().repartition(chunk_size)\n",
    "    \n",
    "    if delta_table_exists(layer, table_name):\n",
    "        print(f\"The Delta table '{table_name}' already exists.\")\n",
    "        \n",
    "        if recreate:\n",
    "            print(f\"Recreating the Delta table '{table_name}'...\")\n",
    "            \n",
    "            # Drop the existing table and remove associated files\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            dbutils.fs.rm(delta_path, recurse=True)\n",
    "            \n",
    "            # Write the new data from the JSON file\n",
    "            json_chunks.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "            \n",
    "            # Register the new table in the Hive metastore\n",
    "            spark.sql(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "            USING DELTA \n",
    "            LOCATION '{delta_path}'\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"The Delta table '{table_name}' has been recreated.\")\n",
    "    else:\n",
    "        print(f\"The Delta table '{table_name}' does not exist. Creating a new table...\")\n",
    "        \n",
    "        # Create a new Delta table\n",
    "        json_chunks.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        \n",
    "        # Register the new table in the Hive metastore\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name}\n",
    "        USING DELTA \n",
    "        LOCATION '{delta_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"The Delta table '{table_name}' has been created.\")\n",
    "        \n",
    "    # Display the first five rows of the Delta table\n",
    "    print(\"Displaying the first five rows of the Delta table...\")\n",
    "    display(spark.read.format(\"delta\").load(delta_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3d1cfc-7bc4-410c-8602-1f6bb37f4852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_update_delta(layer, table_name, data_source, \n",
    "                           join_on=[\"id\", \"last_update\"], recreate=False):\n",
    "    \"\"\"\n",
    "    Create, append, or recreate a Delta table in the specified layer, \n",
    "    register the table in the Hive metastore, and display the \n",
    "    first five rows of the Delta table.\n",
    "\n",
    "    Args:\n",
    "        layer (str): The layer (silver or gold) in which to create/append/recreate the Delta table.\n",
    "        table_name (str): The name of the Delta table.\n",
    "        data_source (DataFrame, optional): The Spark DataFrame to be loaded. \n",
    "                                           If None, the latest Parquet file from the ingestion path is used.\n",
    "        join_on (list, optional): List of columns to join on when deduplicating data. Default is [\"id\", \"last_update\"].\n",
    "        recreate (bool, optional): If True, drop and recreate the existing Delta table. Default is False.\n",
    "\n",
    "    \"\"\"\n",
    "    delta_path = f\"{BASE_PATH}{layer}/delta/{table_name}/\"\n",
    "\n",
    "    if delta_table_exists(layer, table_name):\n",
    "        print(f\"The Delta table '{table_name}' already exists.\")\n",
    "        \n",
    "        if recreate:\n",
    "            print(f\"Recreating the Delta table '{table_name}'...\")\n",
    "            \n",
    "            # Drop the existing Delta table\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            \n",
    "            # Remove the associated files of the Delta table\n",
    "            dbutils.fs.rm(delta_path, recurse=True)\n",
    "            \n",
    "            # Create a new Delta table using the provided DataFrame's schema\n",
    "            data_source.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "            \n",
    "            # Register the new Delta table in the Hive metastore\n",
    "            spark.sql(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "            USING DELTA \n",
    "            LOCATION '{delta_path}'\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"The Delta table '{table_name}' has been recreated.\")\n",
    "        else:\n",
    "            print(f\"Appending new data to the existing Delta table '{table_name}'...\")\n",
    "            \n",
    "            # Load new data\n",
    "            new_data = data_source\n",
    "            \n",
    "            # Load existing data\n",
    "            existing_data = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "            # Deduplicate new data with existing data based on provided columns\n",
    "            new_data = new_data.join(existing_data, join_on, \"left_anti\")\n",
    "\n",
    "            # Append new data to Delta table\n",
    "            new_data.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "            \n",
    "            print(f\"New data has been appended to the Delta table '{table_name}'.\")\n",
    "    else:\n",
    "        print(f\"The Delta table '{table_name}' does not exist. Creating a new table...\")\n",
    "        \n",
    "        # Create the Delta table\n",
    "        data_source.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        \n",
    "        # Register the Delta table in the Hive metastore under 'arxiv' database\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name}\n",
    "        USING DELTA \n",
    "        LOCATION '{delta_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"The Delta table '{table_name}' has been created.\")\n",
    "    \n",
    "    # Display the first five rows of the Delta table\n",
    "    print(\"Displaying the first five rows of the Delta table...\")\n",
    "    display(spark.read.format(\"delta\").load(delta_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba03d2f9-f12f-4034-800b-eea66919417c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01f457c-c7eb-4036-82bc-a619b428e540",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Raw Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c2c67d-3db6-4d5b-b350-ce90accd80bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_or_replace_kaggle_delta(\"silver\", \"raw_kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eca70d6-8591-44f8-9f86-fad74b767632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create DataFrame from Raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d61f5b-90bf-46be-98cd-f329a6340af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.table(\"raw_kaggle\")\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acc83fd-3142-4938-86f9-d49b0133c627",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4319bc97-b55b-4256-a16a-30227f213782",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0564c22a-1895-4066-a107-91008385b60c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter by AI Research Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b7fd58-c522-4e5e-96e3-3506522a50f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the list of categories\n",
    "categories_list = ['cs.MA', 'cs.RO', 'cs.CV', 'cs.LG', 'cs.AI', 'cs.CL', 'cs.NE']\n",
    "\n",
    "# Create a regular expression pattern to match any of the categories\n",
    "categories_pattern = '|'.join(categories_list)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = raw_df.filter(F.col(\"categories\").rlike(categories_pattern))\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acda1cc-b174-4c39-8b02-239e00907783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c5f989-a344-4676-aaaf-56728086a612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Schema Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d8fb68-c3d7-4d1f-9b75-93a6351741f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define UDFs\n",
    "def convert_date(date_str):\n",
    "    if date_str:\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')\n",
    "            dt = pytz.utc.localize(dt)\n",
    "            return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def format_authors(authors_parsed):\n",
    "    authors = []\n",
    "    for author in authors_parsed:\n",
    "        if author[0] and author[1]:\n",
    "            full_name = f\"{author[1]} {author[0]}\"\n",
    "            authors.append(full_name)\n",
    "    return authors\n",
    "\n",
    "convert_date_udf = F.udf(convert_date, StringType())\n",
    "format_authors_udf = F.udf(format_authors, ArrayType(StringType()))\n",
    "\n",
    "# Assuming raw_kaggle is your original DataFrame\n",
    "aligned_df = (filtered_df\n",
    "    .withColumnRenamed(\"abstract\", \"summary\")\n",
    "    .withColumn(\"categories\", F.split(F.col(\"categories\"), \" \"))\n",
    "    .withColumn(\"last_update\", convert_date_udf(F.element_at(F.col(\"versions.created\"), -1)))\n",
    "    .withColumn(\"published_on\", convert_date_udf(F.element_at(F.col(\"versions.created\"), 1)))\n",
    "    .withColumn(\"authors\", format_authors_udf(F.col(\"authors_parsed\")))\n",
    "    .select(\"id\", \"title\", \"summary\", \"authors\", \"categories\", \"published_on\", \"last_update\")\n",
    ")\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "display(aligned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2855d4-b05a-48fe-b56c-ca087ca908f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2708adfe-db50-4bd7-abea-195d301c7508",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f280a7-2fbf-4b1e-83ba-f0d00ec94e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of rows in the original DataFrame\n",
    "original_count = aligned_df[\"id\", \"title\"].count()\n",
    "\n",
    "# Print the original count\n",
    "print(f\"Original row count in the raw DataFrame: {original_count}\")\n",
    "\n",
    "# Drop duplicate rows and count the number of rows\n",
    "deduped_df = aligned_df.dropDuplicates([\"id\", \"title\"])\n",
    "new_count = deduped_df.count()\n",
    "\n",
    "# Check if there were any duplicates\n",
    "if original_count > new_count:\n",
    "    print(f\"There were {original_count - new_count} duplicate rows in the raw DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the raw DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15dcbcb1-2ec2-4ad4-bd63-1dcf4a939140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Drop rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60fdbba-9a72-44a8-87f7-0269cebf69fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate the data to count null values for each column\n",
    "null_counts = deduped_df.agg(*[F.sum(F.when(F.isnull(c), 1).otherwise(0)).alias(c) for c in deduped_df.columns])\n",
    "\n",
    "# Collect the data to the driver (since the result will be small)\n",
    "null_counts_collected = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Flag to check if any column has null values\n",
    "has_nulls = False\n",
    "\n",
    "# Display columns with null values\n",
    "for column, null_count in null_counts_collected.items():\n",
    "    if null_count > 0:\n",
    "        has_nulls = True\n",
    "        print(f\"Column {column} has {null_count} null values.\")\n",
    "\n",
    "# Print message if no null value is found\n",
    "if not has_nulls:\n",
    "    clean_df = deduped_df.dropna()\n",
    "    print(\"No null values found in the DataFrame.\")\n",
    "else:\n",
    "    # Print rows with null values\n",
    "    print(\"\\nRows with null values:\")\n",
    "    conditions = [F.isnull(c) for c in deduped_df.columns]\n",
    "    null_rows = deduped_df.filter(conditions[0])\n",
    "    for condition in conditions[1:]:\n",
    "        null_rows = null_rows.union(deduped_df.filter(condition))\n",
    "    null_rows.show()\n",
    "\n",
    "    # Remove rows with null values\n",
    "    clean_df = deduped_df.dropna()\n",
    "    print(\"\\nRows with null values have been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34170ed-9197-4422-80bf-d3c604af94cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Column conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963aecea-815f-4097-8453-652a70345ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'published_on' column to date type\n",
    "clean_df = clean_df.withColumn(\"published_on\", clean_df[\"published_on\"].cast(\"timestamp\"))\n",
    "\n",
    "# Convert the 'last_update' column to date type\n",
    "clean_df = clean_df.withColumn(\"last_update\", clean_df[\"last_update\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20225035-9588-43dd-a6b3-2d85cf6e59fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc820dc-f0c9-4fa6-b9a5-06a574f208d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ad067f5-7732-4769-a1ed-ba375feff493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create date and time columns from timestamp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f076ccd-3b62-42e5-9565-05d21c053af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split 'published_on' into date and time\n",
    "enriched_df = clean_df.withColumn(\"published_date\", F.to_date(clean_df[\"published_on\"]))\n",
    "enriched_df = enriched_df.withColumn(\"published_time\", F.date_format(enriched_df[\"published_on\"], \"HH:mm:ss\"))\n",
    "\n",
    "# Split 'last_update' into date and time\n",
    "enriched_df = enriched_df.withColumn(\"last_update_date\", F.to_date(enriched_df[\"last_update\"]))\n",
    "enriched_df = enriched_df.withColumn(\"last_update_time\", F.date_format(enriched_df[\"last_update\"], \"HH:mm:ss\"))\n",
    "\n",
    "# If desired, drop the original timestamp columns\n",
    "enriched_df = enriched_df.drop(\"published_on\", \"last_update\")\n",
    "\n",
    "# Display the enriched dataframe\n",
    "display(enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c45aba0-ec92-4719-ada4-80e41d2f8c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create or update Preprocessed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e1bb81-096f-48b9-bdf5-957a3c1910bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_or_update_delta(\"silver\", \"preprocessed\", data_source=enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ede9a48-8fb0-4769-b6e0-e9d90140dd5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Show Preprocessed Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95371893-9818-42d3-b1f5-d8409a23c7f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"preprocessed\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f6b04e-b886-4735-adbf-45c1474dcfee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"preprocessed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff891c-866b-4fde-8c6f-633682ddff3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"preprocessed\").count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "arxiv-bronze-to-silver-kaggle",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
