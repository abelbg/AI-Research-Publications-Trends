{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f627c88-8890-44e6-9c8a-8951d4a231fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Analyzing Trends in AI Research Publication:\n",
    "# From Silver To Gold\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa744149-a2b2-4bf2-9999-b9e9b8cf92da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d00a73-b82e-4e58-80ff-d75199942bae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16e5098-2973-43b5-b007-2ac6e958126d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # spaCy\n",
    "    import os\n",
    "    os.system('pip install nltk')\n",
    "    os.system('python -m nltk.downloader stopwords')\n",
    "\n",
    "    # If all is good, hide output or display success message\n",
    "    print(\"Installation successful.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Display the error\n",
    "    print(f\"Error:{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1d1b1d-3ee8-497d-9d30-f180eb6e24ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de88385-4f6a-466b-b3ca-b4ab34f474fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Arxiv Database in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2eb5bc-91f8-4f81-9735-bbb19b2b765c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the 'arxiv' database exists or create it\n",
    "if not spark.catalog.databaseExists(\"arxiv\"):\n",
    "    spark.sql(\"CREATE DATABASE arxiv\")\n",
    "\n",
    "# Switch to the 'arxiv' database\n",
    "spark.sql(\"USE arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb585d8-f77f-461a-8669-826126dedcb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80860e1-735c-432e-ab65-c895e69ff288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/mnt/arxiv/\"\n",
    "INGESTION_PATH = \"/mnt/arxiv/bronze/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa799b9-5ed5-4a75-9400-62bd6c4b2f02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d70165-5ede-4c87-a6c1-08008e726430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_latest_parquet():\n",
    "    \"\"\"\n",
    "    Load the most recent Parquet file into a DataFrame\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(INGESTION_PATH) \n",
    "    parquet_files = [f.name for f in files if f.name.endswith('.parquet')]\n",
    "    sorted_files = sorted(parquet_files, reverse=True)\n",
    "    latest_file = sorted_files[0]\n",
    "    return spark.read.parquet(INGESTION_PATH + latest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64cf4a8-8f6d-4585-8095-993ce81f3645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delta_table_exists(layer, table_name):\n",
    "    \"\"\"\n",
    "    Check if Delta table exists\n",
    "    \"\"\"\n",
    "    table_path = f\"{BASE_PATH}{layer}/delta/{table_name}/_delta_log/\"\n",
    "    try:\n",
    "        dbutils.fs.ls(table_path) # Try to read 1 byte from the _delta_log directory\n",
    "        return True\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f454fcfa-0dd2-4d45-a76f-45c02586ec63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_update_delta(layer, table_name, data_source=None, \n",
    "                           join_on=[\"id\", \"last_update_date\"], recreate=False):\n",
    "    \"\"\"\n",
    "    Create, append, or recreate a Delta table in the specified layer, \n",
    "    register the table in the Hive metastore, and display the \n",
    "    first five rows of the Delta table.\n",
    "\n",
    "    Args:\n",
    "        layer (str): The layer (silver or gold) in which to create/append/recreate the Delta table.\n",
    "        table_name (str): The name of the Delta table.\n",
    "        data_source (DataFrame, optional): The Spark DataFrame to be loaded. \n",
    "                                           If None, the latest Parquet file from the ingestion path is used.\n",
    "        join_on (list, optional): List of columns to join on when deduplicating data. Default is [\"id\", \"last_update\"].\n",
    "        recreate (bool, optional): If True, drop and recreate the existing Delta table. Default is False.\n",
    "\n",
    "    \"\"\"\n",
    "    delta_path = f\"{BASE_PATH}{layer}/delta/{table_name}/\"\n",
    "\n",
    "    def load_data():\n",
    "        if data_source is None:\n",
    "            print(\"Loading data from the latest Parquet file...\")\n",
    "            return load_latest_parquet()\n",
    "        else:\n",
    "            print(\"Using provided DataFrame as data source...\")\n",
    "            return data_source  # Assuming data_source is a DataFrame\n",
    "\n",
    "    if delta_table_exists(layer, table_name):\n",
    "        print(f\"The Delta table '{table_name}' already exists.\")\n",
    "        \n",
    "        if recreate:\n",
    "            print(f\"Recreating the Delta table '{table_name}'...\")\n",
    "            \n",
    "            # Drop the existing Delta table\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            \n",
    "            # Remove the associated files of the Delta table\n",
    "            dbutils.fs.rm(delta_path, recurse=True)\n",
    "            \n",
    "            # Create a new Delta table using the provided DataFrame's schema\n",
    "            load_data().write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "            \n",
    "            # Register the new Delta table in the Hive metastore\n",
    "            spark.sql(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "            USING DELTA \n",
    "            LOCATION '{delta_path}'\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"The Delta table '{table_name}' has been recreated.\")\n",
    "        else:\n",
    "            print(f\"Appending new data to the existing Delta table '{table_name}'...\")\n",
    "            \n",
    "            # Load new data\n",
    "            new_data = load_data()\n",
    "            \n",
    "            # Load existing data\n",
    "            existing_data = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "            # Deduplicate new data with existing data based on provided columns\n",
    "            new_data = new_data.join(existing_data, join_on, \"left_anti\")\n",
    "\n",
    "            # Append new data to Delta table\n",
    "            new_data.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "            \n",
    "            print(f\"New data has been appended to the Delta table '{table_name}'.\")\n",
    "    else:\n",
    "        print(f\"The Delta table '{table_name}' does not exist. Creating a new table...\")\n",
    "        \n",
    "        # This is the first run\n",
    "        initial_data = load_data()\n",
    "        \n",
    "        # Create the Delta table\n",
    "        initial_data.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        \n",
    "        # Register the Delta table in the Hive metastore under 'arxiv' database\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name}\n",
    "        USING DELTA \n",
    "        LOCATION '{delta_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"The Delta table '{table_name}' has been created.\")\n",
    "    \n",
    "    # Display the first five rows of the Delta table\n",
    "    print(\"Displaying the first five rows of the Delta table...\")\n",
    "    display(spark.read.format(\"delta\").load(delta_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f846bd97-2f68-472f-bb46-df39c3d5fee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f43864-ee71-4aac-9d84-4bbe3d46cc85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create DataFrame from Preprocessed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cbd30ef-6ab1-40d3-8c07-88d9c4fad541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_df = spark.table(\"preprocessed\")\n",
    "preprocessed_df = preprocessed_df.where(preprocessed_df.published_date >= \"2010-01-01\")\n",
    "display(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ed13f7-cc24-42c3-a975-a73e7b425a7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Group by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b118bf-8fbc-4957-b60d-3ee415a468cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Explode Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cf3535-41e7-4741-9ba1-9f4721a7d5b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categories_exploded_df = preprocessed_df.select(\"*\", F.explode(preprocessed_df.categories).alias(\"category\"))\n",
    "\n",
    "num_unique_categories = categories_exploded_df.select(\"category\").distinct().count()\n",
    "\n",
    "print(f\"Numbers of unique categories in dataset: {num_unique_categories}\")\n",
    "\n",
    "display(categories_exploded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6135616c-fd08-4cb7-82ba-f1e296c11630",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter by AI-related categories only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b613a107-61e3-46f8-9102-79829b2a3ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the categories to be considered\n",
    "selected_categories = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.MA', 'cs.NE', 'cs.RO']\n",
    "\n",
    "# Filter DataFrame based on the selected categories\n",
    "categories_filtered_df = categories_exploded_df.where(F.col('category').isin(selected_categories))\n",
    "\n",
    "display(categories_filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b2d152-d741-4ac0-ab6a-8f46c29513e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Number of publications by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7329c290-d1a5-44b4-bddc-9e07f191f502",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cff1190-aaa4-4352-a4d3-67fbf4b9781f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "publications_by_category_unfiltered_df = categories_exploded_df.groupBy(\"category\").count().orderBy('count', ascending=False)\n",
    "\n",
    "create_or_update_delta(\"gold\", \"publications_by_category_unfiltered\", data_source=publications_by_category_unfiltered_df, join_on=[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bde0803-e211-4242-af10-1685f948a706",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0967d381-612f-43bf-9b00-4a0f05280358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mapping for category renaming\n",
    "category_mapping = {\n",
    "    'cs.AI': \"Artificial Intelligence\",\n",
    "    'cs.CL': \"Computational Linguistics\",\n",
    "    'cs.CV': \"Computer Vision\",\n",
    "    'cs.LG': \"Machine Learning\",\n",
    "    'cs.MA': \"Multiagent Systems\",\n",
    "    'cs.NE': \"Neural and Evolutionary Computing\",\n",
    "    'cs.RO': \"Robotics\"\n",
    "}\n",
    "\n",
    "# Construct the renaming logic\n",
    "expr = F.col(\"category\")\n",
    "for arxiv_code, description in category_mapping.items():\n",
    "    expr = F.when(F.col(\"category\") == arxiv_code, description).otherwise(expr)\n",
    "\n",
    "# Apply the renaming\n",
    "categories_filtered_df = categories_filtered_df.withColumn(\"category\", expr)\n",
    "\n",
    "publications_by_category_filtered_df = categories_filtered_df.groupBy(\"category\").count().orderBy('count', ascending=False)\n",
    "\n",
    "create_or_update_delta(\"gold\", \"publications_by_category_filtered\", data_source=publications_by_category_filtered_df, join_on=[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe17b97-128d-4391-b206-eb85fa44f458",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Group by Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "942273df-4dc2-450f-9c66-88c1bd128b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Explode Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c0a69c-ce14-4eff-9e78-a8fadf5a2d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode the authors column to create a new row for each author of each paper\n",
    "authors_exploded_df = preprocessed_df.select(\"id\", \"categories\", F.explode(preprocessed_df.authors).alias(\"author\"))\n",
    "\n",
    "num_unique_authors = authors_exploded_df.select(\"author\").distinct().count()\n",
    "\n",
    "print(f\"Number of unique authors in dataset: {num_unique_authors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e12eb52-e459-4214-9a18-36b0c606534b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Number of publications by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9414dd71-f570-4209-94e4-e38591a26d93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Group by the author and count the number of papers\n",
    "publications_by_author_df = authors_exploded_df.groupBy(\"author\").count().orderBy('count', ascending=False)\n",
    "\n",
    "create_or_update_delta(\"gold\", \"publications_by_author\", data_source=publications_by_author_df, join_on=[\"author\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13441800-aded-4c1e-a199-17f3a04614ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Group by Publication Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd63e74-4ee1-4ea3-ad51-5353ec866f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Number of publications by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f67fd42-e280-4ad3-8af4-4f8ba2b41374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by the 'published_date' column and count the number of papers\n",
    "publications_by_date_df = preprocessed_df.groupBy(\"published_date\").count().orderBy(\"published_date\")\n",
    "\n",
    "create_or_update_delta(\"gold\", \"publications_by_date\", data_source=publications_by_date_df, join_on=[\"published_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7432740-0a8f-41a7-b073-687fb941ce94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Number of publications by category by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3afab9f-b029-49dc-8b5a-c059a8f4a139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'category' and 'published_date' columns and count the number of papers\n",
    "publications_by_category_by_date_df = categories_filtered_df.groupBy('category', 'published_date').count().orderBy('published_date', 'category')\n",
    "\n",
    "create_or_update_delta(\"gold\", \"publications_by_category_by_date\", data_source=publications_by_category_by_date_df, join_on=[\"category\", \"published_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0593d91a-c46b-4c0e-bb40-e50b976921c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ea64de-bbbb-4a3c-b2fa-19fb7c87522e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a52e60c-3a12-487d-b01a-3739ef5a9902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a set of English Stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c009ed3c-9e94-4207-813d-963d1d7380ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### On Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2da6568-c4d6-4ebb-96b7-ba9124135c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the titles and explode to create a row for each word\n",
    "words_df = preprocessed_df.withColumn('word', F.explode(F.split(F.lower(F.col('title')), '\\\\W+')))  # split by non-word characters to avoid punctuation\n",
    "\n",
    "# Filter out stopwords and words with length less than 2\n",
    "filtered_words_df = words_df.filter(~F.col('word').isin(stop_words)).filter(F.length(F.col('word')) > 1)\n",
    "\n",
    "# Compute word frequencies\n",
    "word_freq_title_df = filtered_words_df.groupBy('word').count().orderBy('count', ascending=False)\n",
    "\n",
    "create_or_update_delta(\"gold\", \"word_freq_title\", data_source=word_freq_title_df, join_on=[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194e58cd-c71c-4212-9425-d73835a37604",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### On Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23db851a-c820-4a05-a881-37fd26e503fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the summaries and explode to create a row for each word\n",
    "words_df = preprocessed_df.withColumn('word', F.explode(F.split(F.lower(F.col('summary')), '\\\\W+')))  # split by non-word characters to avoid punctuation\n",
    "\n",
    "# Filter out stopwords and words with length less than 2\n",
    "filtered_words_df = words_df.filter(~F.col('word').isin(stop_words)).filter(F.length(F.col('word')) > 1)\n",
    "\n",
    "# Compute word frequencies\n",
    "word_freq_summary_df = filtered_words_df.groupBy('word').count().orderBy('count', ascending=False)\n",
    "\n",
    "create_or_update_delta(\"gold\", \"word_freq_summary\", data_source=word_freq_summary_df, join_on=[\"word\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "arxiv-silver-to-gold",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
